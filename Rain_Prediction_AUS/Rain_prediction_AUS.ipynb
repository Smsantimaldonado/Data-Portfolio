{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Rain Prediction for Australia\n",
        "This project was performed as the Final Project for the Data Scientist certification at Coderhouse academy.\n",
        "\n",
        "The dataset was extracted from Kaggle: https://www.kaggle.com/datasets/arunavakrchakraborty/australia-weather-data\n",
        "\n",
        "## Audience\n",
        "Every person or entity that wants to know if tomorrow will rain or not in Australia. This audience includes agricultural, insurance, food and other companies that need to plan their operations based on the weather.\n",
        "\n",
        "## Commercial context\n",
        "Weather services in their free version offer a very vague system: it is based on repetitions of the same climate variables at a similar stage in previous periods.\n",
        "\n",
        "## Main goal\n",
        "Determine if it will rain the next day, in different cities in Australia, without relying solely on the repetition of values in the variables.\n",
        "\n",
        "## Analytical context\n",
        "There is data on temperature, humidity level, winds, presence of clouds/sun and atmospheric pressure for more than 100.000 days, from different cities in Australia.\n",
        "\n",
        "## Secondary questions\n",
        "1. What is the most important variable to predict whether it will rain tomorrow or not? And the 5 most important?\n",
        "2. Is there any variable that is infallible (>80% correlation) when predicting rain?\n",
        "3. What is the wind that most influences the probability of rain the next day?\n",
        "4. What relationship is there between the extreme T° (maximum - minimum) with the time at which the T° are taken?\n",
        "5. Given that we have some measurements taken at 2 times, the measurements at which times are more important for predicting tomorrow's rain?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 229,
      "metadata": {
        "id": "OuvBZybKBR8J"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uCW3ULH5ddqt"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import datetime as dt\n",
        "import seaborn as sns\n",
        "import matplotlib as mpl\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "import plotly.offline as py\n",
        "import plotly.graph_objs as go\n",
        "import plotly.express as px\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import learning_curve\n",
        "import statsmodels.formula.api as sm\n",
        "from geopy.geocoders import Nominatim\n",
        "import pypyodbc as podbc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K761bGisEEOG"
      },
      "outputs": [],
      "source": [
        "conn = podbc.connect(\"Driver={ODBC Driver 17 for SQL Server};\"\n",
        "                     \"Server=ACER-SWIFT-3-SF\\SQLEXPRESS;\"\n",
        "                     \"Database=weatherAUS;\"\n",
        "                     \"Trusted_Connection=yes;\")\n",
        "\n",
        "weather = pd.read_sql_query('''SELECT * FROM [dbo].[weather_AUS]''', conn)\n",
        "\n",
        "conn.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Idgn0bWFEEOI"
      },
      "outputs": [],
      "source": [
        "weather.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exploration section\n",
        "`Date` - Date of the record\n",
        "\n",
        "`Location` - City name\n",
        "\n",
        "`MinTemp` - The minimum temperature during a particular day `[Celsius degrees]`\n",
        "\n",
        "`MaxTemp` - The maximum temperature during a particular day `[Celsius degrees]`\n",
        "\n",
        "`Lluvia` - Rain during a particular day `[millimeters]`\n",
        "\n",
        "`Evaporación` - Evaporation during a particular day `[millimeters]`\n",
        "\n",
        "`Sunshine` - Bright sun during a particular day `[hours]`\n",
        "\n",
        "`WindGusDir` - The direction of the strongest wind gust during a particular day `[16 points of the compass]`\n",
        "\n",
        "`WindGustSpeed` ​​​​- Speed of the strongest wind gust during a particular day `[kilometers per hour]`\n",
        "\n",
        "`WindDir9am` - Wind direction during the 10 previous minutes to 9 am `[cardinal points]`\n",
        "\n",
        "`WindDir3pm` - Wind direction during the 10 previous minutes to 3:00 pm `[cardinal points]`\n",
        "\n",
        "`WindSpeed ​​9am` - Wind speed during the 10 previous minutes to 9 am `[kilometers per hour]`\n",
        "\n",
        "`WindSpeed3pm` - Wind speed during the 10 previous minutes to 3 pm `[kilometers per hour]`\n",
        "\n",
        "`Humidity9am` - Wind humidity at 9 am `[percetage]`\n",
        "\n",
        "`Humidity3pm` - Wind humidity at 3 pm `[percetage]`\n",
        "\n",
        "`Presión9am` - Atmospheric pressure at 9 am `[hectopascles]`\n",
        "\n",
        "`Presión3pm` - Atmospheric pressure at 3 pm `[hectopascles]`\n",
        "\n",
        "`Cloud9am` - portion of the sky obscured by clouds at 9 am `[eighths of sky]`\n",
        "\n",
        "`Cloud3pm` - portion of the sky obscured by clouds at 3 pm `[eighths of sky]`\n",
        "\n",
        "`Temp9am` - Temperature at 9am `[Celsius degrees]`\n",
        "\n",
        "`Temp3pm` - Temperature at 3 pm `[Celsius degrees]`\n",
        "\n",
        "`RainToday` - If it rains today, then value is 1 `[yes]`. If it does not rain today, then value is 0 `[no]`\n",
        "\n",
        "`RainTomorrow` - If it rains tomorrow, then value is 1 `[yes]`. If it does not rain tomorrow, then value is 0 `[no]`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tY2MFZDYi2tc"
      },
      "source": [
        "The color and letter format for graphics is defined"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tE3_hPHX9cTy"
      },
      "outputs": [],
      "source": [
        "colours = ['#f2a73d', '#f76157', '#f6daab', '#dabd7b', '#f04155', '#ff823a', '#f2f26f', '#fff7bd', '#95cfb7', '#f40034',\n",
        "           '#07f9a2', '#09c184', '#0a8967', '#0c5149', '#0d192b', '#fe1cac', '#820081', '#e4b302', '#e7204e', '#3f2c26']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cHjrxRjJhFNi"
      },
      "outputs": [],
      "source": [
        "axtitle_dict = {'family': 'serif', 'color': 'darkred', 'weight': 'bold', 'size': 12}\n",
        "axlab_dict = {'family': 'serif', 'color': 'black', 'size': 10}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-1BTf38aKS5-"
      },
      "outputs": [],
      "source": [
        "weather.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTlh5Fq6jCJ7"
      },
      "source": [
        "Since they are very different numbers, we see them as a percentage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JC9I773ThRNw"
      },
      "outputs": [],
      "source": [
        "round(weather.isnull().sum()*100/weather.shape[0], 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qE1Vt4kYg4M0"
      },
      "source": [
        "At a micro level, here I face the first challenge: columns with many nulls - some with over 40% even.\n",
        "\n",
        "On the other hand, the target column has only 2.25% of null values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Ji9wjKVP1I3"
      },
      "outputs": [],
      "source": [
        "import missingno as msno"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "msno.matrix(weather)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Yq_kMnW7ejK"
      },
      "outputs": [],
      "source": [
        "weather.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7WG_BhI0QL3o"
      },
      "outputs": [],
      "source": [
        "weather.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M4K14ZL8QUyW"
      },
      "outputs": [],
      "source": [
        "weather.describe().round(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aE91tDxUjvbV"
      },
      "source": [
        "Once that exploration is finished, we designate the date column as date (it was found as text before)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ni3b5BZxUY5y"
      },
      "outputs": [],
      "source": [
        "weather.date = pd.DatetimeIndex(weather.date)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMcBpLH5x_wz"
      },
      "source": [
        "The pressure is indicated in Hectopascals so we will change the units to atmospheric units\n",
        "\n",
        "1[ATM] = 1013.25 [HECTOPASCALS]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d-XbJ8iyxoaF"
      },
      "outputs": [],
      "source": [
        "hp_atm = 1013.25\n",
        "weather['pressure9am'] = weather['pressure9am'] / hp_atm\n",
        "weather['pressure3pm'] = weather['pressure3pm'] / hp_atm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_oEXT9_YTfk"
      },
      "source": [
        "> # Feature Engineering: Null values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zu6tGnZA3DPq"
      },
      "source": [
        "## Numeric columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sjJBtlvM0ZBE"
      },
      "source": [
        "We sort the dataset 'prioritizing' first by city and then by date"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2z6X2KEB0TQi"
      },
      "outputs": [],
      "source": [
        "weather = weather.sort_values(by=['location', 'date'], axis=0, ascending=[True, True])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79TsKw6SwYiN"
      },
      "source": [
        "1. Numeric columns containing null/NaN values are as follows:\n",
        "`mintemp`, `maxtemp`, `rainfall`, `evaporation`, `sunshine`, `windgustspeed`, `windspeed9am`, `windspeed3pm`, `humidity9am`, `humidity3pm`, `pressure9am`, `pressure3pm`, `cloud9am`, `cloud3pm`, `temp9am`, `temp3pm`.\n",
        "\n",
        "In order to fill the null values of the numerical columns listed previously, an iterative imputation model will be used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yp_tQg4jwxxp"
      },
      "outputs": [],
      "source": [
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "51jIN7Q7qhXY"
      },
      "outputs": [],
      "source": [
        "numeric_features = ['mintemp', 'maxtemp', 'rainfall', 'evaporation', 'sunshine', 'windgustspeed', 'windspeed9am', 'windspeed3pm',\n",
        "                     'humidity9am', 'humidity3pm', 'pressure9am', 'pressure3pm', 'cloud9am', 'cloud3pm', 'temp9am', 'temp3pm']\n",
        "\n",
        "pct_nulls = round(weather.loc[:, numeric_features].isnull().sum()*100/weather.loc[:, numeric_features].shape[0], 1)\n",
        "pct_nulls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4R_TQE1UN8zX"
      },
      "source": [
        "Every column with a number of null values greater than 35% will be eliminated since estimating so many null data would input significant bias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4zvE1P2cNmFd"
      },
      "outputs": [],
      "source": [
        "to_delete = pct_nulls[pct_nulls > 35].index.to_list()\n",
        "to_delete"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vzVP0CWhPqM8"
      },
      "outputs": [],
      "source": [
        "weather.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bKfN38GoPhCU"
      },
      "outputs": [],
      "source": [
        "weather.drop(columns=to_delete, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C6B9e21IPtI1"
      },
      "outputs": [],
      "source": [
        "weather.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ub-dgnqtQNw9"
      },
      "outputs": [],
      "source": [
        "numeric_features = [x for x in numeric_features if x not in to_delete]\n",
        "numeric_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RyBVbI_ZRlUM"
      },
      "outputs": [],
      "source": [
        "round(weather.loc[:, numeric_features].isnull().sum()*100/weather.loc[:, numeric_features].shape[0], 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x8d2r99gHJZp"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(15,6))\n",
        "cor = weather.loc[:, numeric_features].corr()\n",
        "ax = sns.heatmap(cor, annot=True, fmt='.1f')\n",
        "ax.set_title('HEATMAP DE CORRELACIÓN', fontdict=axtitle_dict)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IiOGobWnKBQW"
      },
      "source": [
        "According to the correlations between features, other features will be imputed with an iterative method, separating those that have the greatest correlation with each other"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEI3Zs1_GMTA"
      },
      "source": [
        "### Iterative Imputer: T°"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lTq-KNc8ml3K"
      },
      "outputs": [],
      "source": [
        "temps = ['mintemp', 'maxtemp', 'temp9am', 'temp3pm']\n",
        "Xnum_temps = weather.loc[:, temps].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S1vV0jM8qU05"
      },
      "outputs": [],
      "source": [
        "print('Nulls (before): %d' % sum(np.isnan(Xnum_temps).flatten()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KaFZ21vFqedY"
      },
      "outputs": [],
      "source": [
        "imputer = IterativeImputer(sample_posterior=True, imputation_order=\"ascending\", random_state=123)\n",
        "imputer.fit(Xnum_temps)\n",
        "Xtrans_temps = imputer.transform(Xnum_temps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PMzF5yqprE3C"
      },
      "outputs": [],
      "source": [
        "print('Nulls (after): %d' % sum(np.isnan(Xtrans_temps).flatten()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aVbJYeO2vOva"
      },
      "outputs": [],
      "source": [
        "weather.loc[:, temps] = Xtrans_temps\n",
        "round(weather[temps].isnull().sum()*100/weather[temps].shape[0], 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWP-vWcqGTOC"
      },
      "source": [
        "### Iterative Imputer: Wind Speeds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bk7SObsaGTOO"
      },
      "outputs": [],
      "source": [
        "speeds = ['windgustspeed', 'windspeed9am', 'windspeed3pm']\n",
        "Xnum_speeds = weather.loc[:, speeds].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LDTHnkyTGTOP"
      },
      "outputs": [],
      "source": [
        "print('Nulls (before): %d' % sum(np.isnan(Xnum_speeds).flatten()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RBjIEpW8GTOP"
      },
      "outputs": [],
      "source": [
        "imputer = IterativeImputer(sample_posterior=True, imputation_order=\"ascending\", random_state=123)\n",
        "imputer.fit(Xnum_speeds)\n",
        "Xtrans_speeds = imputer.transform(Xnum_speeds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e8FgEOW-GTOQ"
      },
      "outputs": [],
      "source": [
        "print('Nulls (after): %d' % sum(np.isnan(Xtrans_speeds).flatten()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r_Tru8dtGTOQ"
      },
      "outputs": [],
      "source": [
        "weather.loc[:, speeds] = Xtrans_speeds\n",
        "round(weather[speeds].isnull().sum()*100/weather[speeds].shape[0], 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGmVaU5LG_k6"
      },
      "source": [
        "### Iterative Imputer: humidity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lAOfufQRG_lC"
      },
      "outputs": [],
      "source": [
        "sch = ['humidity9am', 'humidity3pm']\n",
        "Xnum_sch = weather.loc[:, sch].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "683uUDuVG_lC"
      },
      "outputs": [],
      "source": [
        "print('Nulls (before): %d' % sum(np.isnan(Xnum_sch).flatten()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S7TyCLVxG_lD"
      },
      "outputs": [],
      "source": [
        "imputer = IterativeImputer(sample_posterior=True, imputation_order=\"ascending\",\n",
        "                           random_state=123)\n",
        "imputer.fit(Xnum_sch)\n",
        "Xtrans_sch = imputer.transform(Xnum_sch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rHw8_IPMG_lD"
      },
      "outputs": [],
      "source": [
        "print('Nulls (after): %d' % sum(np.isnan(Xtrans_sch).flatten()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Ql5tLkzG_lE"
      },
      "outputs": [],
      "source": [
        "weather.loc[:, sch] = Xtrans_sch\n",
        "round(weather[sch].isnull().sum()*100/weather[sch].shape[0], 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jl2zz6jMLPCD"
      },
      "source": [
        "### Iterative Imputer: pressure"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZq03hSwSDWz"
      },
      "source": [
        "In this case 3 variables would be repeated: `mintemp`, `windgustspeed`, `temp9am`.\n",
        "\n",
        "Since these 3 variables have already been imputed, they will not undergo modifications and will only be used to fill in the other 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jsYa5lp6LPCM"
      },
      "outputs": [],
      "source": [
        "pressures = ['pressure9am', 'pressure3pm', 'mintemp', 'windgustspeed', 'temp9am']\n",
        "Xnum_pressures = weather.loc[:, pressures].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z_2BxVDULPCM"
      },
      "outputs": [],
      "source": [
        "print('Nulls (before): %d' % sum(np.isnan(Xnum_pressures).flatten()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YzmOSXcuLPCN"
      },
      "outputs": [],
      "source": [
        "imputer = IterativeImputer(sample_posterior=True, imputation_order=\"ascending\", random_state=123)\n",
        "imputer.fit(Xnum_pressures)\n",
        "Xtrans_pressures = imputer.transform(Xnum_pressures)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "go6YOqf1LPCN"
      },
      "outputs": [],
      "source": [
        "print('Nulls (after): %d' % sum(np.isnan(Xtrans_pressures).flatten()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QhE7A7mWLPCN"
      },
      "outputs": [],
      "source": [
        "weather.loc[:, pressures] = Xtrans_pressures\n",
        "round(weather[pressures].isnull().sum()*100/weather[pressures].shape[0], 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rx56b0olSuDa"
      },
      "source": [
        "### Iterative Imputer: rainfall"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eB0Tsn5ZSuDh"
      },
      "source": [
        "To fill in the `rainfall` values we will use all the variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oeWGRupTSuDi"
      },
      "outputs": [],
      "source": [
        "rain = ['rainfall']\n",
        "Xnum_rain = weather.loc[:, numeric_features].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hJzGr_5PSuDi"
      },
      "outputs": [],
      "source": [
        "print('Nulls (before): %d' % sum(np.isnan(Xnum_rain).flatten()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ELHLc0vDSuDi"
      },
      "outputs": [],
      "source": [
        "imputer = IterativeImputer(sample_posterior=True, imputation_order=\"ascending\", random_state=123)\n",
        "imputer.fit(Xnum_rain)\n",
        "Xtrans_rain = imputer.transform(Xnum_rain)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nm76rrgESuDi"
      },
      "outputs": [],
      "source": [
        "print('Nulls (after): %d' % sum(np.isnan(Xtrans_rain).flatten()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oda-AHpkSuDj"
      },
      "outputs": [],
      "source": [
        "weather.loc[:, numeric_features] = Xtrans_rain\n",
        "round(weather[rain].isnull().sum()*100/weather[rain].shape[0], 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iX2PS9WqSuDj"
      },
      "source": [
        "Now all the numerical fields included in the list have 0% null values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4woeJ_Ld3I9K"
      },
      "source": [
        "## Categorical columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drN-X7Kz4jfx"
      },
      "source": [
        "Empty character fields: `windgustdir`, `winddir9am`, `winddir3pm`.\n",
        "\n",
        "This fields were filled with the frequent values (the one that is repeated the most) for each column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8_ffCFqwdhAi"
      },
      "outputs": [],
      "source": [
        "winds_directions = ['windgustdir', 'winddir9am', 'winddir3pm']\n",
        "\n",
        "for col in winds_directions:\n",
        "  print(weather[col].value_counts())\n",
        "  print('-------------------------------')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GY8lza63-VhI"
      },
      "outputs": [],
      "source": [
        "for col in winds_directions:\n",
        "  weather[col] = weather[col].fillna(weather[col].value_counts().idxmax())\n",
        "\n",
        "round(weather[winds_directions].isnull().sum()*100/weather[winds_directions].shape[0], 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-5_6GK6X-V4"
      },
      "source": [
        "In the original dataset, every null row for the field `raintoday` is always equal to the value in the `raintommorrow` column, but the previous row (a day before)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fY6RukGaAnIK"
      },
      "source": [
        "I created a function that checks, for each day and each city, if the milimeters registered (`rainfall`) are more than 0. In case they are, the `raintoday` column is filled with 'Yes'.\n",
        "\n",
        "Then the `raintomorrow` field is populated with the data on the following row as explained before (if the field `raintoday`, the row i+1 is representing that it rained on that particular day, the the row i for the column `raintomorrow`, should be 'yes')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sardw6280pMx"
      },
      "outputs": [],
      "source": [
        "weather.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "hc8q0sNOP6vB"
      },
      "outputs": [],
      "source": [
        "def check_value(row):\n",
        "  if row['rainfall'] != 0:\n",
        "    return 'Yes'\n",
        "  elif row['raintoday'] == 0:\n",
        "    return 'No'\n",
        "\n",
        "weather['raintoday'] = weather['raintoday'].fillna(weather.apply(check_value, axis=1))\n",
        "weather['raintomorrow'] = weather['raintomorrow'].fillna(weather['raintoday'].shift(-1))\n",
        "\n",
        "round(weather.isnull().sum()*100/weather.shape[0], 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CFsa1Wo10LNJ"
      },
      "outputs": [],
      "source": [
        "weather.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQFniPuDAq80"
      },
      "source": [
        "The last row is eliminated since there's no a following row to complete the `raintomorrow` field, and that field can't be empty or with null values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QAzTjJ6_A3uh"
      },
      "outputs": [],
      "source": [
        "weather.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3s5_G4Yth2yL"
      },
      "outputs": [],
      "source": [
        "weather.dropna(inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GEbZoqeUA_5r"
      },
      "outputs": [],
      "source": [
        "weather.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "He4l753eHHyW"
      },
      "source": [
        "> # Feature Engineering: location"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I8nVsFBJm0Ex"
      },
      "outputs": [],
      "source": [
        "list_0 = ['PearceRAAF', 'NorfolkIsland', 'Launceston', 'Hobart']\n",
        "\n",
        "for i in list_0:\n",
        "  weather = weather.drop(weather[weather['location'] == i].index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5jYLH-u_2wko"
      },
      "outputs": [],
      "source": [
        "weather.shape[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2CMP-VlwD633"
      },
      "source": [
        "Every row that has values for the cities of PearceRAAF, Norfolk Island, Launceston and Hobart, are eliminated since those are in continental Australia and data is not reliable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZFAs_o4JY5Ds"
      },
      "outputs": [],
      "source": [
        "windspeed_weather = weather.groupby(['location'])[['windspeed9am', 'windspeed3pm', 'windgustspeed']].mean()\n",
        "windspeed_weather = windspeed_weather.reset_index()\n",
        "windspeed_weather.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5jSC6XqqIAd"
      },
      "source": [
        "We will reduce the `location` field. Now we have 49 cities and we will make have the 6 states of Australia.\n",
        "\n",
        "To do so, I create a 'df' dataframe where the city is followed by \", Australia\" and then use the OpenStreetMaps 'Nominative' API with the GeoPy library to obtain the latitude and longitude of each city. With this data I classify the different cities in each state. Below we will see the distribution of the cities analyzed in these states (Univariate Analysis section)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Lz3POL53kcO"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame()\n",
        "df['ciudades'] = weather['location'].unique()\n",
        "\n",
        "import re\n",
        "def separar_palabras(cadena):\n",
        "  patron = r'(?<!^)(?=[A-Z])'\n",
        "  palabras = re.sub(patron, ' ', cadena)\n",
        "  return palabras\n",
        "\n",
        "df['separada'] = df['ciudades'].apply(separar_palabras)\n",
        "\n",
        "df['separada'] = df['separada'].apply(lambda ciudad: str(ciudad + ', Australia'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TN8W5p6V_-bV"
      },
      "outputs": [],
      "source": [
        "def obtener_latitud(ciudad):\n",
        "  geolocator = Nominatim(user_agent=\"rain_prediction_AUS\", timeout=10)\n",
        "  location = geolocator.geocode(ciudad)\n",
        "  if location is None:\n",
        "    return None\n",
        "  else:\n",
        "    return location.latitude"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4tUPyjPrKajS"
      },
      "outputs": [],
      "source": [
        "def obtener_longitud(ciudad):\n",
        "  geolocator = Nominatim(user_agent=\"rain_prediction_AUS\", timeout=10)\n",
        "  location = geolocator.geocode(ciudad)\n",
        "  if location is None:\n",
        "    return None\n",
        "  else:\n",
        "    return location.longitude"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SBxbVWcxJ329"
      },
      "outputs": [],
      "source": [
        "df['lat'] = df['separada'].apply(obtener_latitud)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cUpH7FwJKapf"
      },
      "outputs": [],
      "source": [
        "df['lon'] = df['separada'].apply(obtener_longitud)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HKyRWk6wHsjL"
      },
      "outputs": [],
      "source": [
        "citys = df['ciudades'].tolist()\n",
        "latitudes = df['lat'].tolist()\n",
        "longitudes = df['lon'].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9sTnTKGpLmrw"
      },
      "outputs": [],
      "source": [
        "diccionario_latitudes = dict(zip(citys, latitudes))\n",
        "weather['Latitud'] = weather['location'].map(diccionario_latitudes)\n",
        "\n",
        "diccionario_longitudes = dict(zip(citys, longitudes))\n",
        "weather['Longitud'] = weather['location'].map(diccionario_longitudes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2rljni0QPjnq"
      },
      "outputs": [],
      "source": [
        "mask_wa = df['lon'] < 129\n",
        "df.loc[mask_wa, 'State'] = 'Western Australia'\n",
        "\n",
        "mask_nt = (df['lat'] > -26) & (df['lon'] > 129) & (df['lon'] < 138)\n",
        "df.loc[mask_nt, 'State'] = 'Northern Territory'\n",
        "\n",
        "mask_sa = (df['lat'] < -26) & (df['lon'] > 129) & (df['lon'] < 141)\n",
        "df.loc[mask_sa, 'State'] = 'South Australia'\n",
        "\n",
        "mask_nsw = (df['lat'] < -29) & (df['lon'] > 141)\n",
        "df.loc[mask_nsw, 'State'] = 'New South Wales'\n",
        "\n",
        "mask_q = (df['State'] != 'Western Australia') & (df['State'] != 'Northern Territory') & (df['State'] != 'South Australia') & (df['State'] != 'New South Wales')\n",
        "df.loc[mask_q, 'State'] = 'Queensland'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eH3i0i4Dpekj"
      },
      "source": [
        "Below we can see the distribution of cities for each state by absolute and percetaje values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TL3KMCvQPjhM"
      },
      "outputs": [],
      "source": [
        "print(df['State'].value_counts())\n",
        "print('---------------------------')\n",
        "print(round(df['State'].value_counts()*100/df['State'].shape[0], 2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1kRJRJsvPja-"
      },
      "outputs": [],
      "source": [
        "estados = df['State'].tolist()\n",
        "diccionario_estados = dict(zip(citys, estados))\n",
        "weather['location'] = weather['location'].map(diccionario_estados)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fxhTY7xePjW6"
      },
      "outputs": [],
      "source": [
        "print(weather['location'].value_counts())\n",
        "print('----------------------------')\n",
        "print(round(weather['location'].value_counts()*100/weather['location'].shape[0], 2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_2UZvHgqRAO"
      },
      "source": [
        "Las proporciones vemos que se mantiene de forma aproximada"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgUjg-jGHHef"
      },
      "source": [
        "> # Feature Engineering: Outliers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x-pTjqdsXNac"
      },
      "outputs": [],
      "source": [
        "weather.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B5sriNI2XbNF"
      },
      "outputs": [],
      "source": [
        "categorical_features = []\n",
        "for col in weather.columns:\n",
        "\tif weather[col].dtype == 'object':\n",
        "\t\tcategorical_features.append(col)\n",
        "\n",
        "print(f'Columnas categoricas son {categorical_features}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A-qtLrQFXerG"
      },
      "outputs": [],
      "source": [
        "print(f'Columnas numéricas son {numeric_features}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPVDUTTbwl4t"
      },
      "source": [
        "**Outliers** will be handled with **IQR** method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kiWub0kf7uPg"
      },
      "outputs": [],
      "source": [
        "weather_num = weather[numeric_features]\n",
        "weather_num.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JopIBQopHIt9"
      },
      "outputs": [],
      "source": [
        "Q1 = weather_num.quantile(0.25)\n",
        "Q3 = weather_num.quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "((weather_num < (Q1 - 1.5 * IQR)) | (weather_num > (Q3 + 1.5 * IQR))).any()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8r4W3ZG_HiXT"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=[24, 16])\n",
        "fig.suptitle('Boxplots', fontsize=18, fontweight = 'bold')\n",
        "fig.subplots_adjust(top = 0.92)\n",
        "fig.subplots_adjust(hspace = 0.4, wspace = 0.23)\n",
        "for i, col in enumerate(numeric_features):\n",
        "  ax1 = fig.add_subplot(4, 4, i+1)\n",
        "  ax1 = sns.boxplot(data = weather, x=col, color= colours[i])\n",
        "  ax1.set_title(f'{col}', fontdict=axtitle_dict)\n",
        "  ax1.set_xlabel(f'{col}', fontdict=axlab_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxjqfxvOroPJ"
      },
      "source": [
        "`evaporation` and `rainfall` features has most of it values around or equal to 0. This is expected due to the fact that is less likely that it rains in a given day.\n",
        "\n",
        "In the features that there's involved the wind speed, there are a lot of outliers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vEEjrjBsHuTF"
      },
      "outputs": [],
      "source": [
        "diccionario = {}\n",
        "for col in list(numeric_features):\n",
        "  percentile25 = weather[col].quantile(0.25)\n",
        "  percentile75 = weather[col].quantile(0.75)\n",
        "  IQR  = percentile75 - percentile25\n",
        "  upper_limit = percentile75 + 1.5 * IQR\n",
        "  lower_limit = percentile25 - 1.5 * IQR\n",
        "  diccionario['upper_limit'+ '_' + col] = upper_limit\n",
        "  diccionario['lower_limit'+ '_' + col] = lower_limit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bd3ECcC4ISSG"
      },
      "outputs": [],
      "source": [
        "for col in list(numeric_features):\n",
        "    weather[col] = np.where(\n",
        "        weather[col] > diccionario['upper_limit_' + col],\n",
        "        diccionario['upper_limit_' + col],\n",
        "        np.where(\n",
        "            weather[col] < diccionario['lower_limit_' + col],\n",
        "            diccionario['lower_limit_' + col],\n",
        "            weather[col]\n",
        "       )\n",
        "   )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XMn-hw_HIsXq"
      },
      "outputs": [],
      "source": [
        "# Verificando\n",
        "fig = plt.figure(figsize=[24, 16])\n",
        "fig.suptitle('Boxplot de data corregida', fontsize=18, fontweight = 'bold')\n",
        "fig.subplots_adjust(top = 0.92)\n",
        "fig.subplots_adjust(hspace = 0.4, wspace = 0.15)\n",
        "for i, col in enumerate(list(numeric_features)):\n",
        "    ax1 = fig.add_subplot(5, 3, i+1)\n",
        "    ax1 = sns.boxplot(data = weather, x=col, color= colours[i])\n",
        "    ax1.set_title(f'{col}', fontdict=axtitle_dict)\n",
        "    ax1.set_xlabel(f'{col}', fontdict=axlab_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qf4gQ4dHTzt"
      },
      "source": [
        "> # Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLbu299JuMiV"
      },
      "source": [
        "## Univariate analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bhnI7lu3GWLZ"
      },
      "outputs": [],
      "source": [
        "count = weather['date'].dt.year.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PlHmrDPGHhOG"
      },
      "outputs": [],
      "source": [
        "ax = sns.barplot(x = count.index, y = count.values)\n",
        "ax.set_title(f'Cant de registros por año', fontdict=axtitle_dict)\n",
        "ax.set_xlabel('Año', fontdict=axlab_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g6pgV9lDM5ul"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=[24, 10])\n",
        "\n",
        "fig.subplots_adjust(top = 0.92)\n",
        "fig.subplots_adjust(hspace = 0.25, wspace = 0.23)\n",
        "for i, columns in enumerate(winds_directions):\n",
        "  input = np.unique(weather[columns], return_counts = True)\n",
        "  col= 'input'\n",
        "  ax1 = fig.add_subplot(1, 3, i+1)\n",
        "  ax1 = sns.barplot(x=list(eval(f'{col}[0]')), y=list(eval(f'{col}[1]')))\n",
        "  ax1.set_title(f'{columns}', fontdict=axtitle_dict)\n",
        "  ax1.set_xlabel(f'{columns}', fontdict=axlab_dict)\n",
        "  ax1.set_ylabel('Count', fontdict=axlab_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nvDzM5PAfutZ"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=[20, 20])\n",
        "fig.suptitle('Categorical fields - Count Plot', fontsize=18, fontweight = 'bold')\n",
        "fig.subplots_adjust(top = 0.92)\n",
        "fig.subplots_adjust(hspace = 0.25, wspace = 0.23)\n",
        "for i, columns in enumerate(categorical_features):\n",
        "  input = np.unique(weather[columns], return_counts = True)\n",
        "  col= 'input'\n",
        "  ax1 = fig.add_subplot(3, 2, i+1)\n",
        "  ax1 = sns.barplot(x=list(eval(f'{col}[0]')), y=list(eval(f'{col}[1]')))\n",
        "  ax1.set_title(f'{columns}', fontdict=axtitle_dict)\n",
        "  ax1.set_xlabel(f'{columns}', fontdict=axlab_dict)\n",
        "  ax1.set_ylabel('Count', fontdict=axlab_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JC9gkfO5CBfv"
      },
      "source": [
        "Se ve que las clases están desbalanceadas... en especial las variables `raintoday` y `raintomorrow`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uc7TL7dICcwJ"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=[24, 18])\n",
        "fig.suptitle('Numerical fields - distribution', fontsize=18, fontweight = 'bold')\n",
        "fig.subplots_adjust(top = 0.95)\n",
        "fig.subplots_adjust(hspace = 0.25, wspace = 0.25)\n",
        "for i, col in enumerate(numeric_features):\n",
        "  ax = fig.add_subplot(4, 4, i+1)\n",
        "  ax = sns.distplot(weather[col], color=colours[i])\n",
        "  ax.axvline(weather[col].quantile(q=0.25), color = 'green', linestyle = '--', label = '25% Quartile')\n",
        "  ax.axvline(weather[col].mean(), color = 'red', linestyle = '--', label = 'Mean')\n",
        "  ax.axvline(weather[col].median(), color = 'black', linestyle = '--', label = 'Median')\n",
        "  ax.axvline(weather[col].quantile(q=0.75), color = 'blue', linestyle = '--', label = '75% Quartile')\n",
        "  ax.set_xlabel(f'{col}', fontdict=axlab_dict)\n",
        "  ax.set_title(f'{col.upper()}', fontdict=axtitle_dict)\n",
        "  ax.legend(fontsize=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NW4f2mCqsr8w"
      },
      "source": [
        "Conclusions:\n",
        "\n",
        "1. In Australia the most common wind is from the West, from the Indian Ocean. This wind is called `Monzon` and generates strong tropical rainfall in the summer, while dry and cold weather in the winter. This generates that the state called 'Western Australia' (15% of the dataset) has 2 opposite weather conditions throughout the year.\n",
        "\n",
        "2. This makes particular sense when you consider that in the morning, the predominant winds are from the north, and in the afternoon, they come from the southeast. The winds shift across the western front during the 9 am to 15pm hour time span.\n",
        "\n",
        "3. The feature `rainfall` becomes almost like a binary TRUE or FALSE flag, similar to the `raintoday` feature. Most days indicate that it did NOT rain, which is logical. Therefore, it would be interesting to know what happened on the day following those when it DID rain.\n",
        "\n",
        "4. The variables that account for cloud cover exhibit very peculiar behavior with nearly impossible jumps, appearing almost uniformly distributed. It seems like a uniform distribution, but with very large jumps that are almost evenly spaced (in values) from one another.\n",
        "\n",
        "5. The following variables: `rainfall`, `humidity9am`, `humidity3pm`, and `wind speeds` can't be negative. For all the negative values detected (which appear to be in small quantities based on the graphs), they will be replaced with 0 since it represents the minimum possible value for these variables.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CTl1Z6C6Bo6l"
      },
      "outputs": [],
      "source": [
        "list_no_nulos = ['rainfall', 'humidity9am', 'humidity3pm', 'windgustspeed', 'windspeed9am', 'windspeed3pm']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gRt5g4juQTgA"
      },
      "outputs": [],
      "source": [
        "for col in list_no_nulos:\n",
        "  weather[col] = weather[col].apply(lambda x: 0 if x < 0 else x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m_DM99JKEAh9"
      },
      "outputs": [],
      "source": [
        "x = windspeed_weather.loc[:, 'location']\n",
        "y1 = windspeed_weather['windspeed9am']\n",
        "y2 = windspeed_weather['windspeed3pm']\n",
        "y3 = windspeed_weather['windgustspeed']\n",
        "\n",
        "plt.figure(figsize = (15, 8))\n",
        "\n",
        "plt.plot(x, y2, color = 'darkorange', label = 'WindSpeed at 3pm')\n",
        "plt.plot(x, y1, color = 'green', label = 'WindSpeed at 9am', linestyle='--')\n",
        "plt.plot(x, y3, color = 'blue', label = 'WindGustSpeed')\n",
        "\n",
        "plt.xlabel('Ciudad', fontdict=axlab_dict)\n",
        "plt.ylabel('Velocidad [km/h]', fontdict=axlab_dict)\n",
        "plt.title('Promedio de velocidad de vientos por Ciudad', fontdict=axtitle_dict)\n",
        "plt.legend(fontsize = 10, loc = 'best')\n",
        "plt.xticks(rotation=90)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K4OxWxmZEAnI"
      },
      "outputs": [],
      "source": [
        "df = weather.copy()\n",
        "df['Week'] = df['date'].dt.week\n",
        "\n",
        "tupla = df.groupby(['Week']).mean()['rainfall']\n",
        "df = pd.DataFrame(tupla)\n",
        "fig=px.line(data_frame=df, x=df.index, y='rainfall', title='Promedio Semanal de Precipitaciones', labels={'Week': 'Semana ','rainfall': 'Promedio de lluvia [mm] '})\n",
        "fig.update_layout(paper_bgcolor='#FFFFFF',plot_bgcolor='#FFFFFF')\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xcik9HzHuFd8"
      },
      "source": [
        "## Bivariate analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qhi40YBM94q"
      },
      "source": [
        "Let's see the distribution of cities across Australia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ciBvyHlf8FX7"
      },
      "outputs": [],
      "source": [
        "import folium\n",
        "from folium.plugins import HeatMap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "leTA_hAV0Zv1"
      },
      "outputs": [],
      "source": [
        "folium_hmap = folium.Map(location=[-26, 134], zoom_start=5, tiles=\"OpenStreetMap\")\n",
        "\n",
        "datos = list(zip(weather['Latitud'], weather['Longitud']))\n",
        "\n",
        "hm_wide = HeatMap(data = datos, min_opacity=0.8,\n",
        "                  radius=10, blur=6,  max_zoom=10)\n",
        "\n",
        "folium_hmap.add_child(hm_wide)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUJUvS8-86un"
      },
      "source": [
        "The distance between cities in really big. This can be a problem in the future for this project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aUnLjY5uuXT9"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=[24, 10])\n",
        "fig.suptitle('Bivariate analysis of Winds Direction', fontsize=18, fontweight = 'bold')\n",
        "fig.subplots_adjust(top = 0.9)\n",
        "fig.subplots_adjust(hspace = 0.25, wspace = 0.2)\n",
        "for i, col in enumerate(list(winds_directions)):\n",
        "  a = fig.add_subplot(1, 3, i+1)\n",
        "  a = sns.countplot(x = weather[col], ax = a, hue = weather.raintomorrow, color= colours[i])\n",
        "  a.set_title(col, fontdict=axtitle_dict)\n",
        "  a.legend(fontsize=15)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2EZXc7fvrf1"
      },
      "source": [
        "There's a highly variable relationship between the incidence of the direction from which the rain comes and whether it ends up raining or not. For example, the directions ENE and SSW have a very similar number of samples in the dataset (as seen in the univariate analysis), but when the wind comes from the SSW direction, it rains 50% more often compared to when it comes from the ENE direction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vJ6NfIrp1CEa"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=[24, 16])\n",
        "fig.suptitle('Bivariate analysis of numerical fields', fontsize=18, fontweight = 'bold')\n",
        "fig.subplots_adjust(top = 0.9)\n",
        "fig.subplots_adjust(hspace = 0.4, wspace = 0.3)\n",
        "\n",
        "for i, col in enumerate(list(numeric_features)):\n",
        "  a = fig.add_subplot(4, 4, i+1)\n",
        "  a=sns.boxplot(data = weather, x = 'raintomorrow', y =col, ax=a)\n",
        "  a.set_title(col, fontdict=axtitle_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OeuXMU907Pbe"
      },
      "outputs": [],
      "source": [
        "f, axs = plt.subplots(1, 2, figsize=(15, 8))\n",
        "sns.scatterplot(data=weather, x=\"maxtemp\", y=\"mintemp\", hue=\"location\", alpha=.5, ax=axs[0])\n",
        "sns.histplot(data=weather, x=\"location\", hue=\"location\", shrink=.9, alpha=.5,\n",
        "             legend=True, ax=axs[1])\n",
        "f.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eg8JlTOrJppr"
      },
      "outputs": [],
      "source": [
        "contingency_table = pd.crosstab(weather['raintoday'], weather['raintomorrow'])\n",
        "sns.heatmap(contingency_table, annot=True)\n",
        "plt.title('Martiz de escenarios', fontdict=axtitle_dict)\n",
        "plt.xlabel('raintomorrow', fontdict=axlab_dict)\n",
        "plt.ylabel('raintoday', fontdict=axlab_dict)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vCBr2WVHJZh9"
      },
      "source": [
        "## Multivariate analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r1_WwZNl5Aug"
      },
      "outputs": [],
      "source": [
        "weather['raintoday'] = weather['raintoday'].map({'Yes': 1, 'No': 0})\n",
        "weather['raintomorrow'] = weather['raintomorrow'].map({'Yes': 1, 'No': 0})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "THfiVMfeMpKu"
      },
      "outputs": [],
      "source": [
        "list_5 = numeric_features + ['raintoday', 'raintomorrow']\n",
        "\n",
        "correlaciones = round(weather.loc[:, list_5].corr()['raintomorrow'].sort_values(ascending = False)[1:], 2)\n",
        "correlaciones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1qzkTd6QJanh"
      },
      "outputs": [],
      "source": [
        "ax = weather.loc[:, list_5].corr()['raintomorrow'].sort_values(ascending = False)[1:].plot(kind = 'barh', color = '#09c184', figsize = (10, 8))\n",
        "ax.set_title('Correlacion con raintomorrow', fontsize=18, fontweight = 'bold')\n",
        "ax.axvline(0, color = 'black')\n",
        "ax.axvline(0.15, color = '#0a8967', linestyle = 'dotted')\n",
        "ax.axvline(-0.15, color = '#0a8967', linestyle = 'dotted')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-f_qr9mipGYS"
      },
      "outputs": [],
      "source": [
        "list_1 = ['date', 'Latitud', 'Longitud']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V00rmMa_qMwY"
      },
      "outputs": [],
      "source": [
        "list_3 = list_1 + categorical_features + list(weather.location.unique())\n",
        "list_3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zf9GAioVGxB-"
      },
      "outputs": [],
      "source": [
        "list_3 = list_1 + categorical_features + list(weather.location.unique())\n",
        "list_6 = [x for x in list(weather.columns) if x not in list_3] + ['raintoday']\n",
        "list_6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xQQhkJWq7PGF"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(15,6))\n",
        "cor = weather.loc[:, list_6].corr()\n",
        "ax = sns.heatmap(cor, annot=True, fmt='.2f')\n",
        "ax.set_title('HEATMAP DE CORRELACIÓN', fontdict=axtitle_dict)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eLH9oyv5F__1"
      },
      "source": [
        "> # Feature Engineering: Encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITYp50ncHiQf"
      },
      "source": [
        "## One Hot Encoding: location"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "opH7QhhYJhiO"
      },
      "outputs": [],
      "source": [
        "one_hot_encoded = pd.get_dummies(weather['location'], drop_first=False)\n",
        "one_hot_encoded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-JkiIoZVKp60"
      },
      "outputs": [],
      "source": [
        "weather = pd.concat([weather, one_hot_encoded], axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sY0shaluRlA1"
      },
      "source": [
        "## Label Encoder: Vientos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMkRf8bowFGh"
      },
      "source": [
        "A list is created where all the different directions from the three variables that include wind directions are added. Then, only the initials of each of these are kept, which will indicate whether the wind is coming from the North, South, East, or West."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RIe8yDJKwgbW"
      },
      "source": [
        "For each value, we only get the 1st letter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HwxOzc1QuTM1"
      },
      "outputs": [],
      "source": [
        "for col in winds_directions:\n",
        "  print(weather[col].value_counts())\n",
        "  print('-------------------------------')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m5TlRa7E6uwP"
      },
      "outputs": [],
      "source": [
        "for i in winds_directions:\n",
        "  weather[i] = weather[i].apply(lambda dir: dir[0])\n",
        "\n",
        "for col in winds_directions:\n",
        "  print(weather[col].value_counts())\n",
        "  print('-------------------------------')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NqJte9Mp54Ga"
      },
      "outputs": [],
      "source": [
        "series = {}\n",
        "df_aux = pd.DataFrame()\n",
        "\n",
        "for col in winds_directions:\n",
        "    # Obtener el conteo de valores para cada columna\n",
        "    serie = weather[col].value_counts()\n",
        "    # Almacenar la Serie en el diccionario\n",
        "    series[col] = serie\n",
        "\n",
        "# Imprimir las Series resultantes\n",
        "for col, serie in series.items():\n",
        "  df_aux[col] = serie\n",
        "\n",
        "df_aux"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WCh5csU86NCg"
      },
      "outputs": [],
      "source": [
        "total_direc = df_aux.sum(axis=1)\n",
        "orden_direc = total_direc.sort_values(ascending=True)\n",
        "print(orden_direc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AApdUXKb7UXN"
      },
      "outputs": [],
      "source": [
        "list_orden_direc_viento = list(orden_direc.index)\n",
        "list_orden_direc_viento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1XBZ-Ohwrjc"
      },
      "source": [
        "Se crea un diccionario agregandole un numero a cada uno de los posibles valores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YhwvXbQ5Wxt7"
      },
      "outputs": [],
      "source": [
        "diccionario = {direc_viento: i+1 for i, direc_viento in enumerate(list_orden_direc_viento)}\n",
        "diccionario"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UtCiQu9ApyYI"
      },
      "outputs": [],
      "source": [
        "for i in winds_directions:\n",
        "  weather[i] = weather[i].map(diccionario)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eLEi4VV9Yqza"
      },
      "outputs": [],
      "source": [
        "weather[winds_directions]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2z79YvqrMIS"
      },
      "source": [
        "> # Different model combinations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5cIMKuehYhS8"
      },
      "source": [
        "Different combinations between class balancing algorithms and classification algorithms will be used.\n",
        "\n",
        "2 machine learning classification models:\n",
        "* Logistic regression\n",
        "* Decision Tree Classifier\n",
        "\n",
        "3 different options to balance classes:\n",
        "* No balancear las clases\n",
        "* ADASYN\n",
        "* SMOTE\n",
        "\n",
        "\n",
        "We end up having 6 different combinations. For each of this combinations, changes in the hyper-parameters were tested but on this notebook there's only the best result for each of this combinations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0PwHd1nytzwl"
      },
      "source": [
        "> ## Logistic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWteq4VnrMIS"
      },
      "source": [
        "### Feature selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NzjjjIEyER2_"
      },
      "source": [
        "Before selecting variables, we will reset the index. Why? As we will see below, the largest index remains 145460 despite not having that many records as a result of removing rows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NTdx14EQE50F"
      },
      "outputs": [],
      "source": [
        "max(weather.index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3-Hy7OGHERAE"
      },
      "outputs": [],
      "source": [
        "weather.reset_index(drop=True, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_1XvCPaUEys7"
      },
      "outputs": [],
      "source": [
        "max(weather.index)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_pW5xR5uvLd"
      },
      "source": [
        "To select the variables, the SequentialFeatureSelector (SFS) method is used.\n",
        "\n",
        "It is a \"wrapper\" type technique that seeks to find the best subset of features by evaluating different combinations of features and selecting those that improve the model's performance.\n",
        "\n",
        "It can be used in combination with any classification model and can be applied for both forward selection and backward selection.\n",
        "\n",
        "In this case, forward selection will be used because it has shown better results than backward selection method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eqd-kdpPvlyd"
      },
      "outputs": [],
      "source": [
        "from mlxtend.feature_selection import SequentialFeatureSelector as SFS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vNxTgq7qrMIS"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import joblib\n",
        "sys.modules['sklearn.externals.joblib'] = joblib\n",
        "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
        "from sklearn.linear_model import LogisticRegression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZtAMa_zkrMIT"
      },
      "outputs": [],
      "source": [
        "sfs = SFS(LogisticRegression(), k_features=12, forward=True, floating=False, scoring = 'accuracy', cv = 5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c879OBx4rMIT"
      },
      "outputs": [],
      "source": [
        "X_lr = weather.drop(['raintomorrow', 'date', 'Longitud', 'Latitud', 'location'], axis=1)\n",
        "y_lr = weather['raintomorrow']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OAJCYuy3rMIT"
      },
      "outputs": [],
      "source": [
        "sfs.fit(X_lr, y_lr)\n",
        "variables_elegidas = list(sfs.k_feature_names_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ex048bSSCYV0"
      },
      "outputs": [],
      "source": [
        "variables_elegidas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gj7UG1ZkrMIU"
      },
      "source": [
        "### Train/test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g4Y6_g7-rMIU"
      },
      "outputs": [],
      "source": [
        "X_lr = weather[variables_elegidas]\n",
        "y_lr = weather['raintomorrow']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E4gckexfrMIU"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_lr_train, X_lr_test, y_lr_train, y_lr_test = train_test_split(X_lr, y_lr, test_size = 0.3, random_state = 123, shuffle=True, stratify=y_lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6XiZajqLrMIU"
      },
      "outputs": [],
      "source": [
        "X_lr_train.shape, y_lr_train.shape, X_lr_test.shape, y_lr_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xl0xsDIDZ_bJ"
      },
      "outputs": [],
      "source": [
        "y_lr.value_counts()[1]/y_lr.value_counts()[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F-gbKfWeZ_bV"
      },
      "outputs": [],
      "source": [
        "y_lr_train.value_counts()[1]/y_lr_train.value_counts()[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zeyF5XK_Z_bW"
      },
      "outputs": [],
      "source": [
        "y_lr_test.value_counts()[1]/y_lr_test.value_counts()[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqpYVvB4UxMa"
      },
      "source": [
        "### Class balancing - ADASYN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2W9JoBPUxMb"
      },
      "source": [
        "In the feature `raintomorrow`, the classes were unbalanced, which can potentially cause issues in the future.\n",
        "\n",
        "In this case, the `ADASYN (Adaptive Synthetic Sampling)` technique is used. Unlike other oversampling techniques like `SMOTE (Synthetic Minority Over-sampling Technique)`, `ADASYN` adaptively adjusts to the distribution of the minority class. Specifically, this algorithm works through the following steps:\n",
        "\n",
        "1. It calculates the difference between the number of instances in the majority class and the minority class (referred to as the class unbalance difference).\n",
        "\n",
        "2. For each instance in the minority class, it calculates the distance between that instance and its k nearest neighbors from the majority class.\n",
        "\n",
        "3. It computes the sample weights (or proportions) for each instance in the minority class based on the class unbalance difference and the distances to its neighbors.\n",
        "\n",
        "4. Synthetic instances are generated for instances in the minority class using the calculated sample weights. This \"generation of synthetic instances\" is achieved by interpolating features between a minority class instance and its selected k neighbors.\n",
        "\n",
        "Essentially, ADASYN generates synthetic instances near regions of the minority class that are harder to classify, therefore improving the model's ability to capture patterns and characteristics of the minority class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IaHl3J5MUxMb"
      },
      "outputs": [],
      "source": [
        "from imblearn.over_sampling import ADASYN\n",
        "\n",
        "adasyn = ADASYN(n_neighbors=5, sampling_strategy=.43, random_state = 123)\n",
        "X_lr_resampled, y_lr_resampled = adasyn.fit_resample(X_lr_train, y_lr_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vf-OB_tl0cdk"
      },
      "outputs": [],
      "source": [
        "y_lr_resampled.value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55NjxOGz-keG"
      },
      "source": [
        "The amount of values ib the minority class were increased until 43% of the majority class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syGaxtJ1-rov"
      },
      "source": [
        "**Let's see the ML model and classification report using the ADASYN algorithm for the resampling and LOGISTIC REGRESSION algorithm for the predictions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6AwMXK_UrMIX"
      },
      "outputs": [],
      "source": [
        "model_lr = LogisticRegression(max_iter=10000)\n",
        "model_lr.fit(X_lr_resampled, y_lr_resampled)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0NVERCpRrMIX"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "y_lr_pred = model_lr.predict(X_lr_test)\n",
        "print(classification_report(y_true=y_lr_test, y_pred=y_lr_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvrwLOYQuG6C"
      },
      "source": [
        "### Class balancing - SMOTE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwACKUE_uG6C"
      },
      "source": [
        "In this case, we will use the `SMOTE (Synthetic Minority Over-sampling Technique)` technique. It is an oversampling technique that involves generating new synthetic samples for the minority class from existing observations by linear interpolation between the nearest neighbors of the same class.\n",
        "\n",
        "It works by taking a sample from the minority class to find its nearest neighbors. Then, one of these neighbors is randomly chosen, and a synthetic example is created somewhere between the original example and the chosen neighbor. This way, the size of the minority class is increased by creating new samples that are a combination of existing samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vOX2Ik0TuG6C"
      },
      "outputs": [],
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "smote = SMOTE(sampling_strategy = .45)\n",
        "X_lr_resampled, y_lr_resampled = smote.fit_resample(X_lr_train, y_lr_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bopoi5zFuG6C"
      },
      "outputs": [],
      "source": [
        "y_lr_resampled.value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uDArOoOd_KVZ"
      },
      "source": [
        "The amount of values ib the minority class were increased until 45% of the majority class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IAuk1QIZ_Eyu"
      },
      "source": [
        "**Let's see the ML model and classification report using the SMOTE algorithm for the resampling and LOGISTIC REGRESSION algorithm for the predcitions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2bPflwvvuG6D"
      },
      "outputs": [],
      "source": [
        "model_lr = LogisticRegression(max_iter=10000)\n",
        "model_lr.fit(X_lr_resampled, y_lr_resampled)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ikdsU-8TuG6D"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "y_lr_pred = model_lr.predict(X_lr_test)\n",
        "print(classification_report(y_true=y_lr_test, y_pred=y_lr_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KOoSRt8eyeKd"
      },
      "source": [
        "### No class balancing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OUelcFdazYvt"
      },
      "source": [
        "In this case there will be no classes balancing.\n",
        "\n",
        "A cross validation method called Stratified K Fold will be used. It works as follows:\n",
        "k parts from the dataset are selected for both train and test portions of the dataset, checking there are no repetitions in the samples selected for each"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VubPmtUsyj13"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "model_lr = LogisticRegression(max_iter=10000)\n",
        "n_folds = 5\n",
        "acum_accuracy=0\n",
        "i=0\n",
        "\n",
        "kf = StratifiedKFold(n_splits=n_folds)\n",
        "\n",
        "for train_index, test_index in kf.split(X_lr, y_lr):\n",
        "  i += 1\n",
        "\n",
        "  X_train, X_test = X_lr.loc[train_index], X_lr.loc[test_index]\n",
        "  y_train, y_test = y_lr.loc[train_index], y_lr.loc[test_index]\n",
        "\n",
        "  model_lr.fit(X_train, y_train)\n",
        "\n",
        "  accuracy = model_lr.score(X_test, y_test)\n",
        "\n",
        "  acum_accuracy += accuracy\n",
        "  accuracy_redondeada = round(accuracy, 5)\n",
        "\n",
        "  print(f\"Accuracy fold n° {i}: {accuracy_redondeada}\")\n",
        "\n",
        "acum_accuracy_redondeada = round(acum_accuracy/n_folds, 5)\n",
        "print(f\"Accuracy promedio: {acum_accuracy_redondeada}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KePh-kqr1rHE"
      },
      "source": [
        "> ## Decision Tree Classfier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Bta3EM8j9eZ"
      },
      "source": [
        "### Feature selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ysv88V7O6cd7"
      },
      "source": [
        "In this case we will the backwards selection since it has proven better results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2sJaIJpfj9eZ"
      },
      "outputs": [],
      "source": [
        "from sklearn import tree\n",
        "from sklearn.tree import DecisionTreeClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TY9gH7PRj9eZ"
      },
      "outputs": [],
      "source": [
        "sfs = SFS(DecisionTreeClassifier(), k_features=10, forward=False, scoring = 'accuracy', cv = 5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qT5l5Zr2j9ea"
      },
      "outputs": [],
      "source": [
        "X_dtc = weather.drop(['raintomorrow', 'date', 'Longitud', 'Latitud', 'location'], axis=1)\n",
        "y_dtc = weather['raintomorrow']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XZqBTnCej9ea"
      },
      "outputs": [],
      "source": [
        "sfs.fit(X_dtc, y_dtc)\n",
        "variables_elegidas = list(sfs.k_feature_names_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5xz1ZDU1rHM"
      },
      "source": [
        "### Train/test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ub0Iu8FF1rHM"
      },
      "outputs": [],
      "source": [
        "X_dtc = weather[variables_elegidas]\n",
        "y_dtc = weather['raintomorrow']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4vAdsjQv1rHM"
      },
      "outputs": [],
      "source": [
        "X_dtc_train, X_dtc_test, y_dtc_train, y_dtc_test = train_test_split(X_dtc, y_dtc, test_size = .3, random_state = 123, shuffle=True, stratify=y_dtc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iMXGLxrh1rHM"
      },
      "outputs": [],
      "source": [
        "X_dtc_train.shape, y_dtc_train.shape, X_dtc_test.shape, y_dtc_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qLfHVAKB7HIg"
      },
      "outputs": [],
      "source": [
        "y_dtc_train.value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHw7neEJ1rHN"
      },
      "source": [
        "### Class balancing - ADASYN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "76A7HVwi1rHN"
      },
      "outputs": [],
      "source": [
        "adasyn = ADASYN(n_neighbors=5, sampling_strategy=0.4, random_state = 123)\n",
        "X_dtc_resampled, y_dtc_resampled = adasyn.fit_resample(X_dtc_train, y_dtc_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CZK73_Z81rHN"
      },
      "outputs": [],
      "source": [
        "y_dtc_resampled.value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lq8IHfjQ_QrR"
      },
      "source": [
        "The amount of values ib the minority class were increased until 40% of the majority class\n",
        "\n",
        "**Let's see the ML model and classification report using the ADASYN algorithm for the resampling and DECISION TREE CLASSIFIER algorithm for the predictions**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mVq8JiN31rHO"
      },
      "outputs": [],
      "source": [
        "clf = DecisionTreeClassifier(random_state=123, criterion = 'entropy', max_depth=10)\n",
        "model_dtc = clf.fit(X_dtc_resampled, y_dtc_resampled)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eEW458nM1rHO"
      },
      "outputs": [],
      "source": [
        "y_dtc_pred = clf.predict(X_dtc_test)\n",
        "print(classification_report(y_true=y_dtc_test, y_pred=y_dtc_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kx4dShKV1rHO"
      },
      "source": [
        "### Class balancing - SMOTE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hq4n1MZq1rHO"
      },
      "outputs": [],
      "source": [
        "smote = SMOTE(sampling_strategy = .41)\n",
        "X_dtc_resampled, y_dtc_resampled = smote.fit_resample(X_dtc_train, y_dtc_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OS4GS2y41rHP"
      },
      "outputs": [],
      "source": [
        "y_dtc_resampled.value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZAjiB-K_iLp"
      },
      "source": [
        "The amount of values ib the minority class were increased until 41% of the majority class\n",
        "\n",
        "**Let's see the ML model and classification report using the SMOTE algorithm for the resampling and DECISION TREE CLASSIFIER algorithm for the predictions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0H4TVEN_1rHP"
      },
      "outputs": [],
      "source": [
        "clf = DecisionTreeClassifier(random_state=123, criterion = 'entropy', max_depth=10)\n",
        "model_dtc = clf.fit(X_dtc_resampled, y_dtc_resampled)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U-y8khfI1rHP"
      },
      "outputs": [],
      "source": [
        "y_dtc_pred = clf.predict(X_dtc_test)\n",
        "print(classification_report(y_true=y_dtc_test, y_pred=y_dtc_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ESTnqPy51rHP"
      },
      "source": [
        "### No class balancing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxM_8KMk1rHP"
      },
      "source": [
        "In this case there will be no classes balancing.\n",
        "\n",
        "A cross validation method called Stratified K Fold will be used. It works as follows:\n",
        "k parts from the dataset are selected for both train and test portions of the dataset, checking there are no repetitions in the samples selected for each"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uxETtQ--1rHQ"
      },
      "outputs": [],
      "source": [
        "clf = DecisionTreeClassifier(random_state=123, criterion = 'entropy', max_depth=10)\n",
        "n_folds = 7\n",
        "acum_accuracy=0\n",
        "i=0\n",
        "\n",
        "kf = StratifiedKFold(n_splits=n_folds)\n",
        "\n",
        "for train_index, test_index in kf.split(X_lr, y_lr):\n",
        "  i += 1\n",
        "\n",
        "  X_train, X_test = X_lr.loc[train_index], X_lr.loc[test_index]\n",
        "  y_train, y_test = y_lr.loc[train_index], y_lr.loc[test_index]\n",
        "\n",
        "  clf.fit(X_train, y_train)\n",
        "\n",
        "  accuracy = clf.score(X_test, y_test)\n",
        "\n",
        "  acum_accuracy += accuracy\n",
        "  accuracy_redondeada = round(accuracy, 5)\n",
        "\n",
        "  print(f\"Accuracy fold n° {i}: {accuracy_redondeada}\")\n",
        "\n",
        "acum_accuracy_redondeada = round(acum_accuracy/n_folds, 5)\n",
        "print(f\"Accuracy promedio: {acum_accuracy_redondeada}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtP8lfW2l83l"
      },
      "source": [
        "> # Best model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tV__5MyIUCwI"
      },
      "source": [
        "While in most models there is a problem of low identification of class 1 or 'yes', the main criteria for choosing the model is the 'accuracy' metric.\n",
        "\n",
        "Since it is a binary classification problem, success is not only when the 'yes' variable is predicted correctly but also when the 'no' variable is predicted correctly, as this implies that the prediction of whether it will rain or not the next day is correct, which is precisely what we are aiming for.\n",
        "\n",
        "The Decision Tree Classifier (DTC) model is the chosen one using ADASYN as the class balancing method for the target variable. It performs best with the following configuration:\n",
        "* Input with only 10 variables selected using the SequentialFeatureSelector backwards method.\n",
        "* Balancing with the ADASYN method using the 5 nearest neighbors and adjusting the class ratio from approximately 0.33 to 0.40.\n",
        "* The DTC performs best when the criterion used is 'entropy,' with a maximum tree depth of 10."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "94gwIuh0ViDK"
      },
      "outputs": [],
      "source": [
        "adasyn = ADASYN(n_neighbors=5, sampling_strategy=0.4, random_state = 123)\n",
        "X_dtc_resampled, y_dtc_resampled = adasyn.fit_resample(X_dtc_train, y_dtc_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FFrkAvX8ViDL"
      },
      "outputs": [],
      "source": [
        "y_dtc_resampled.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nl5AZn9xViDL"
      },
      "outputs": [],
      "source": [
        "clf = DecisionTreeClassifier(random_state=123, criterion = 'entropy', max_depth=10)\n",
        "model_dtc = clf.fit(X_dtc_resampled, y_dtc_resampled)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XdsxwCfprMIZ"
      },
      "source": [
        "## Matriz de confusión"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ub8_wqaj9N14"
      },
      "source": [
        "Let's see the confusion matrix and ROC (receiver operating characteristic curve) for that model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EUpbMpsmrMIZ"
      },
      "outputs": [],
      "source": [
        "matriz = confusion_matrix(y_lr_test, y_lr_pred)\n",
        "matriz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qNpjbPq6rMIa"
      },
      "outputs": [],
      "source": [
        "cm = confusion_matrix(y_lr_test, y_lr_pred)\n",
        "sns.heatmap(cm, annot=True, fmt='d')\n",
        "plt.xlabel('Predicciones')\n",
        "plt.ylabel('Valores reales')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tUHVw6A_rMIa"
      },
      "source": [
        "Note that the testing was conducted on three different models: **Logistic Regression**, **Decision Tree Classifier**, and **Random Forest Classifier**.\n",
        "\n",
        "While similar results were obtained with all three tested models, the Logistic Regression model showed the best results and consumed the least processing time.\n",
        "\n",
        "Additionally, different class balancing proportions were tested, and the best result was achieved with a 65% ratio. In other words, a higher proportion of balance improves class detection but worsens prediction accuracy, and vice versa. This indicates that the samples created using the SMOTE method introduce some bias into the prediction. Lowering the balance proportion increases prediction accuracy but reduces effectiveness in class detection."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3JsY3gN4rMIa"
      },
      "source": [
        "## Curva ROC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7B2sENorMIa"
      },
      "source": [
        "The ROC metric (`Receiver Operating Characteristic`) is used to evaluate and visualize the performance of a binary classification model.\n",
        "It it a graphic representation of the relation between the `True Positive Rate` and the `False Positive Rate` as the classification threshold is adjusted.\n",
        "\n",
        "The X-axis of the ROC metric represents the `FPR`, which is the proportion of negative instances incorrectly classified as positive. The Y-axis represents the `TPR`, which is the proportion of positive instances correctly classified as positive. As the classification threshold changes, different pairs of `TPR` and `FPR` are obtained, resulting, each combination, in a point on the ROC graphic.\n",
        "\n",
        "Ideally, a classification model would have a high `TPR` and a low `FPR`, leading to an ROC curve that approaches the top-left corner of the graph. A larger area under the ROC curve (AUC-ROC) indicates better model performance, where an AUC-ROC **=** 1.0 represents perfect classification, and an AUC-ROC **<=** 0.5 represents random classification.\n",
        "\n",
        "The ROC curve allows visualizing the performance of a classification model, helping to select the optimal threshold for classification and evaluate the accuracy and balance between true positives and false positives of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kDWSA8CQrMIb"
      },
      "outputs": [],
      "source": [
        "y_dtc_prob = model_dtc.predict_proba(X_dtc_test)[:, 1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6Jdjv1nrMIb"
      },
      "source": [
        "The y_prob matrix has n samples and m classes -> each row represents the possibilty of each class\n",
        "\n",
        "The \"`[:, 1]`\" part allow us to obtain the positive class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fLDieSkkrMIc"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "fpr, tpr, thresholds = roc_curve(y_dtc_test, y_dtc_prob)\n",
        "\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(fpr, tpr, color='#f04155', lw=2, label='Curva ROC (área = %0.2f)' % roc_auc)\n",
        "plt.plot([0, 1], [0, 1], color='black', lw=2, linestyle='--')\n",
        "plt.xlabel('Tasa de Falsos Positivos')\n",
        "plt.ylabel('Tasa de Verdaderos Positivos')\n",
        "plt.title('Curva ROC')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YkoR7Ro7biNd"
      },
      "source": [
        "## Conclusions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cl6QeNaqseJo"
      },
      "source": [
        "We observe that using a decision tree performs very well in predicting the value 0, meaning no rain. However, it is not as good at predicting the value 1, on days when it does rain\n",
        "\n",
        "1. Overall the model is correct 83% of the time, not bad at all\n",
        "\n",
        "2. The precision is also good, indicating that the model is well-controlled in terms of range\n",
        "\n",
        "3. Regarding recall, it's found that for the 'NO rain' values (specificity), we have high precision and high recall - something very difficult to achieve. While for the 'YES rain' values (sensitivity), we have good precision but low recall\n",
        "\n",
        "4. Finally, looking at the more comprehensive metric, we have the F1-score, which gives us a very good value for 'NO rain' but a rather poor one for 'YES rain,' indicating few false values for 0 but many false values for 1. This could be due to class imbalance - see the last cell\n",
        "\n",
        "5. Main success of the model: 'class 0' of the `raintomorrow` feature (target variable) has high precision and high recall -> the model detects the class well and is very reliable when predicting its values\n",
        "\n",
        "6. Main problem detected: 'class 1' of the `raintomorrow` feature (target variable) has high precision but low recall -> the model detects the class incorrectly, but when it does, it is very reliable. This could be due to imbalanced test records in the target feature (y_test)\n",
        "\n",
        "7. The model does not change much with the sample split between train and test. Once again, it was tested from 0.2 to 0.4, and the behavior was approximately the same for all variables analyzed when evaluating (precision, recall, F1-score, the number of correct predictions in the confusion matrix, and the area under the ROC curve)\n",
        "\n",
        "8. Balancing classes up to nearly equal levels is detrimental here. The only combination where the model improved its performance by synthetically increasing the number of samples of 'class 1' is when it approaches 40%\n",
        "\n",
        "9. The hyperparameter that has the most impact on the model is the depth allowed for tree creation -> more trees generate better results, with 10 being the optimal depth according to the **'entropy'** measure"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "3odSH5cUUXcE",
        "zu6tGnZA3DPq",
        "4woeJ_Ld3I9K",
        "He4l753eHHyW",
        "UgUjg-jGHHef",
        "GLbu299JuMiV",
        "Xcik9HzHuFd8",
        "vCBr2WVHJZh9",
        "ITYp50ncHiQf",
        "sY0shaluRlA1",
        "TWteq4VnrMIS",
        "Gj7UG1ZkrMIU",
        "cqpYVvB4UxMa",
        "cvrwLOYQuG6C",
        "KOoSRt8eyeKd",
        "3Bta3EM8j9eZ",
        "P5xz1ZDU1rHM",
        "yHw7neEJ1rHN",
        "Kx4dShKV1rHO",
        "ESTnqPy51rHP",
        "XdsxwCfprMIZ",
        "3JsY3gN4rMIa",
        "YkoR7Ro7biNd"
      ],
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
